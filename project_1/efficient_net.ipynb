{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "from tqdm.notebook import tqdm  # For progress bars in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test tensor created on MPS successfully: mps:0\n",
      "MPS is working properly\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Check if MPS (Apple Silicon GPU) is available - thorough checking\n",
    "def get_device():\n",
    "    if not torch.backends.mps.is_available():\n",
    "        print(\"MPS not available - checking why...\")\n",
    "        if not torch.backends.mps.is_built():\n",
    "            print(\"PyTorch not compiled with MPS support. Verify your PyTorch version (needs 1.12+ and proper installation)\")\n",
    "        else:\n",
    "            print(\"PyTorch has MPS support but MPS is not available on this device\")\n",
    "        return torch.device(\"cpu\")\n",
    "    \n",
    "    # MPS is available, but verify we can actually create a tensor on it\n",
    "    try:\n",
    "        # Attempt to create a small tensor on MPS\n",
    "        test_tensor = torch.zeros(1, device=\"mps\")\n",
    "        print(f\"Test tensor created on MPS successfully: {test_tensor.device}\")\n",
    "        print(\"MPS is working properly\")\n",
    "        return torch.device(\"mps\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing MPS: {e}\")\n",
    "        print(\"Falling back to CPU\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "# Get device for training\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Hyperparameters\n",
    "# You can modify these to experiment with different settings\n",
    "\n",
    "# Batch size - larger values are generally more efficient on the GPU\n",
    "batch_size = 128  # Try 64, 128, 256 for efficiency testing\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 15\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "# Standard transforms for training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # EfficientNet expects 224x224 images\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "])\n",
    "\n",
    "# Transforms for validation/testing (no augmentation)\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "Training samples: 90000\n",
      "Validation samples: 90000\n",
      "Test samples: 90000\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "train_dataset = ImageFolder(root='data/train', transform=train_transform)\n",
    "valid_dataset = ImageFolder(root='data/valid', transform=eval_transform)\n",
    "test_dataset = ImageFolder(root='data/test', transform=eval_transform)\n",
    "\n",
    "# Create data loaders - pin_memory=True improves GPU transfer speed\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                         num_workers=4, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, \n",
    "                         num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, \n",
    "                        num_workers=4, pin_memory=True)\n",
    "\n",
    "# Print dataset info\n",
    "class_names = train_dataset.classes\n",
    "print(f\"Classes: {class_names}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(valid_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded EfficientNet B0 with pre-trained weights\n"
     ]
    }
   ],
   "source": [
    "# ## Model Architecture - EfficientNet B0\n",
    "# We'll use a pre-trained EfficientNet B0 model and adapt it to our dataset\n",
    "\n",
    "def create_efficientnet_model(num_classes=10, pretrained=True):\n",
    "    \"\"\"Create and return an EfficientNet B0 model adapted to our dataset\"\"\"\n",
    "    if pretrained:\n",
    "        # Load pre-trained weights\n",
    "        weights = EfficientNet_B0_Weights.DEFAULT\n",
    "        model = efficientnet_b0(weights=weights)\n",
    "        print(\"Loaded EfficientNet B0 with pre-trained weights\")\n",
    "    else:\n",
    "        model = efficientnet_b0(weights=None)\n",
    "        print(\"Initialized EfficientNet B0 with random weights\")\n",
    "    \n",
    "    # Replace the classifier (final layer)\n",
    "    in_features = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.2, inplace=True),\n",
    "        nn.Linear(in_features=in_features, out_features=num_classes),\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model and move it to the device\n",
    "model = create_efficientnet_model(num_classes=num_classes, pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# ## Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler for better convergence\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Training and Validation Functions\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Use tqdm for progress bar\n",
    "    with tqdm(train_loader, desc=\"Training\", leave=False) as t:\n",
    "        for images, labels in t:\n",
    "            # Move to device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            t.set_postfix(loss=loss.item(), acc=100.*correct/total)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, valid_loader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(valid_loader, desc=\"Validation\", leave=False) as t:\n",
    "            for images, labels in t:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Statistics\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                # Update progress bar\n",
    "                t.set_postfix(loss=loss.item(), acc=100.*correct/total)\n",
    "    \n",
    "    epoch_loss = running_loss / len(valid_loader.dataset)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch device: mps:0\n",
      "Model parameters device: mps:0\n"
     ]
    }
   ],
   "source": [
    "# ## Training Loop\n",
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Verify device placement\n",
    "sample_inputs, _ = next(iter(train_loader))\n",
    "sample_inputs = sample_inputs.to(device)\n",
    "print(f\"Input batch device: {sample_inputs.device}\")\n",
    "\n",
    "model_device = next(model.parameters()).device\n",
    "print(f\"Model parameters device: {model_device}\")\n",
    "\n",
    "# Lists to store metrics\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "lrs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e99b52925fc9440783a319992a48fe28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4b6a9427cd4ccb9c78c8cdda4bdbfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Time: 606.43s | LR: 0.001000\n",
      "Train Loss: 0.5713 | Train Acc: 79.92%\n",
      "Valid Loss: 0.4158 | Valid Acc: 85.24%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adbed52f3f15465393bfc22af748f0f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c78414cf9147a58e79d6e7ecf16a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 | Time: 593.97s | LR: 0.001000\n",
      "Train Loss: 0.3965 | Train Acc: 85.96%\n",
      "Valid Loss: 0.3771 | Valid Acc: 86.67%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a24fb6619045f78bb3f064385d555d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7826cd242c4143b35a89d294ed8f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 | Time: 589.66s | LR: 0.001000\n",
      "Train Loss: 0.3287 | Train Acc: 88.43%\n",
      "Valid Loss: 0.3595 | Valid Acc: 87.52%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef9db1732e14bdfa0362f74b1e081d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e56959a094249eeadc3ccea7997841d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 | Time: 602.21s | LR: 0.001000\n",
      "Train Loss: 0.2881 | Train Acc: 89.78%\n",
      "Valid Loss: 0.3631 | Valid Acc: 87.95%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f380298ce4e4ec6a1e05487351db8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b310a7d1c5664048a2871602c3ea36cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 | Time: 605.31s | LR: 0.001000\n",
      "Train Loss: 0.2567 | Train Acc: 90.88%\n",
      "Valid Loss: 0.3561 | Valid Acc: 88.19%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af13a5e634f45d3a3ee2be4701d5972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53d71e9ff63443ba8536308e0eeec51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 | Time: 609.67s | LR: 0.001000\n",
      "Train Loss: 0.2252 | Train Acc: 91.87%\n",
      "Valid Loss: 0.3419 | Valid Acc: 88.61%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08fb42949ae848db905e1d9ab5574261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f65da1667d3477a9fffa8257153064a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 | Time: 585.49s | LR: 0.001000\n",
      "Train Loss: 0.2019 | Train Acc: 92.74%\n",
      "Valid Loss: 0.3665 | Valid Acc: 88.23%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87cfe17afffe444796856c4b479eae60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8b5e0e5034438c9a77c3a962c262ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 | Time: 588.24s | LR: 0.001000\n",
      "Train Loss: 0.1811 | Train Acc: 93.57%\n",
      "Valid Loss: 0.3916 | Valid Acc: 87.94%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12d96e5d3ed4e4ca34362841af3447f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d149841b46ec4e63b7a20db209ef5f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 | Time: 593.59s | LR: 0.001000\n",
      "Train Loss: 0.1661 | Train Acc: 94.05%\n",
      "Valid Loss: 0.3884 | Valid Acc: 88.41%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a60a3b8fa74fc28aaf17a1e0ffe6f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d3b619001d4b2facc2037202f277df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 | Time: 594.28s | LR: 0.000500\n",
      "Train Loss: 0.1487 | Train Acc: 94.70%\n",
      "Valid Loss: 0.3915 | Valid Acc: 88.64%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05448981f4fc439fbcc8ecc5a87a7446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loop over epochs\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Train and validate\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    valid_loss, valid_acc = validate(model, valid_loader, criterion, device)\n",
    "    \n",
    "    # Learning rate scheduler step\n",
    "    scheduler.step(valid_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    lrs.append(current_lr)\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    valid_accuracies.append(valid_acc)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Time: {epoch_time:.2f}s | LR: {current_lr:.6f}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc:.2f}%\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Training completed in {total_time/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Plotting Training Curves\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Loss subplot\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "# Accuracy subplot\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(valid_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# Learning rate subplot\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(lrs)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('efficientnet_training_curves.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Evaluate on Test Set\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    \"\"\"Evaluate model on test set with per-class accuracy\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    class_correct = [0] * num_classes\n",
    "    class_total = [0] * num_classes\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Per-class accuracy\n",
    "            for i in range(labels.size(0)):\n",
    "                label = labels[i]\n",
    "                pred = predicted[i]\n",
    "                if label == pred:\n",
    "                    class_correct[label] += 1\n",
    "                class_total[label] += 1\n",
    "    \n",
    "    # Print overall accuracy\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "    \n",
    "    # Print per-class accuracy\n",
    "    print(\"\\nPer-class accuracy:\")\n",
    "    for i in range(num_classes):\n",
    "        if class_total[i] > 0:\n",
    "            print(f\"Accuracy of {class_names[i]}: {100 * class_correct[i] / class_total[i]:.2f}%\")\n",
    "    \n",
    "    # Return overall accuracy\n",
    "    return 100 * correct / total\n",
    "\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_accuracy = evaluate(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(model, test_loader, class_names, device):\n",
    "    \"\"\"Create and plot confusion matrix\"\"\"\n",
    "    # Get predictions\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Collecting predictions\"):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Plot confusion matrix\n",
    "try:\n",
    "    plot_confusion_matrix(model, test_loader, class_names, device)\n",
    "except ImportError:\n",
    "    print(\"Skipping confusion matrix - seaborn or scikit-learn not installed.\")\n",
    "    print(\"Install with: pip install seaborn scikit-learn\")\n",
    "\n",
    "# ## Save the Model\n",
    "# Save the trained model\n",
    "model_save_path = 'efficientnet_model.pth'\n",
    "torch.save({\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'train_acc': train_accuracies[-1],\n",
    "    'valid_acc': valid_accuracies[-1],\n",
    "    'test_acc': test_accuracy,\n",
    "}, model_save_path)\n",
    "\n",
    "print(f\"Model saved to '{model_save_path}'\")\n",
    "\n",
    "# ## How to Load and Use the Model Later\n",
    "print(\"\\nTo load and use this model later, use the following code:\")\n",
    "print(\"\"\"\n",
    "# Load the model\n",
    "import torch\n",
    "from torchvision.models import efficientnet_b0\n",
    "\n",
    "# Create model architecture\n",
    "model = efficientnet_b0(weights=None)\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.2, inplace=True),\n",
    "    nn.Linear(in_features=1280, out_features=10),\n",
    ")\n",
    "\n",
    "# Load saved weights\n",
    "checkpoint = torch.load('efficientnet_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Use the model\n",
    "# image = preprocess_image(your_image)  # Apply same transforms as during evaluation\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(image)\n",
    "#     _, predicted = torch.max(outputs, 1)\n",
    "#     print(f\"Predicted class: {class_names[predicted.item()]}\")\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
